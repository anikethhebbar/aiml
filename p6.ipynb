{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "[[3, 9], [2, 9], [1, 9], [1, 8], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
      "[[5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
      "[[9, 5], [9, 4], [9, 3], [8, 3], [7, 3], [7, 4], [7, 5], [6, 5], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "\n",
    "# Define actions\n",
    "# Numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "environment_rows = 11\n",
    "environment_columns = 11\n",
    "\n",
    "# Create a 2D numpy array to hold the rewards for each state.\n",
    "# The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
    "rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "rewards[0, 5] = 100.  # Set the reward for the packaging area (i.e., the goal) to 100.\n",
    "\n",
    "# Define aisle locations (i.e., white squares) for rows 1 through 9\n",
    "aisles = {}  # Store locations in a dictionary\n",
    "aisles[1] = [i for i in range(1, 10)]\n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 8)]\n",
    "aisles[3].append(9)\n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range(11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(11)]\n",
    "\n",
    "# Set the rewards for all aisle locations (i.e., white squares)\n",
    "for row_index in range(1, 10):\n",
    "    for column_index in aisles[row_index]:\n",
    "        rewards[row_index, column_index] = -1.\n",
    "\n",
    "# Define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "    # If the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
    "    if rewards[current_row_index, current_column_index] == -1.:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "    # Get a random row and column index\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_columns)\n",
    "    # Continue choosing random row and column indexes until a non-terminal state is identified\n",
    "    # (i.e., until the chosen state is a 'white square').\n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(environment_rows)\n",
    "        current_column_index = np.random.randint(environment_columns)\n",
    "    return current_row_index, current_column_index\n",
    "\n",
    "# Define an epsilon-greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    # If a randomly chosen value between 0 and 1 is less than epsilon,\n",
    "    # then choose the most promising value from the Q-table for this state.\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else:  # Choose a random action\n",
    "        return np.random.randint(4)\n",
    "\n",
    "# Define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "# Define a function that will get the shortest path between any location within the warehouse that\n",
    "# the robot is allowed to travel and the item packaging location.\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "    # Return immediately if this is an invalid starting location\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    else:  # If this is a 'legal' starting location\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        # Continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "        while not is_terminal_state(current_row_index, current_column_index):\n",
    "            # Get the best action to take\n",
    "            action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "            # Move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "        return shortest_path\n",
    "\n",
    "# Define training parameters\n",
    "num_actions = len(actions)\n",
    "q_values = np.zeros((environment_rows, environment_columns, num_actions))\n",
    "epsilon = 0.9  # The percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9  # Discount factor for future rewards\n",
    "learning_rate = 0.9  # The rate at which the agent should learn\n",
    "\n",
    "# Run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "    # Get the starting location for this episode\n",
    "    row_index, column_index = get_starting_location()\n",
    "    # Continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    # (i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        # Choose which action to take (i.e., where to move next)\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        # Perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index  # Store the old row and column indexes\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        # Receive the reward for moving to the new state, and calculate the temporal difference\n",
    "        reward = rewards[row_index, column_index]\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "        # Update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete!')\n",
    "\n",
    "# Display a few shortest paths\n",
    "print(get_shortest_path(3, 9))  # Starting at row 3, column 9\n",
    "print(get_shortest_path(5, 0))  # Starting at row 5, column 0\n",
    "print(get_shortest_path(9, 5))  # Starting at row 9, column 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
